## Facial-Emotion-Recognition

# Objective
This is the capstone project that constituted the final submission for the MIT Data Science Jul-Oct 22 Program. The goal of this project is to use Deep Learning and Artificial Intelligence techniques to create a computer vision model that can accurately detect facial emotions. The model should be able to perform multi-class classification on images of facial expressions, to classify the expressions according to the associated emotio

# Context
Deep Learning has found many applications in predictive tasks relating to unstructured forms
of data: images, text, audio and video. Predictive modeling aims to match human-level
performance on such tasks. There is a specific branch of AI called Affective Computing or
Emotion AI for the study and development of technologies that can read human emotions by means of analyzing body gestures, facial expressions, voice tone, etc. and react appropriately to them.

In the field of human-machine interaction, facial expression recognition is critical. From recent
research, it has been found that as much as 55% of communication of sentiment takes place
through facial expressions and other visual cues. Therefore, training a model to identify facial
emotions accurately is an important step towards the development of emotionally intelligent
behavior in machines with AI capabilities. Automatic facial expression recognition systems could
have many applications, including but not limited to any use case that requires human behavior
understanding, detection of mental disorders, and creating a higher quality of virtual assistant for
customer-facing businesses.

# Dataset

The data set consists of 3 folders, i.e., 'test', 'train', and 'validation'. Each of these folders has four subfolders for the different emotions to be recognized: ‘happy’, ‘sad’, ‘surprise’ and ‘neutral’.
The images are RGB “white and black” images of size 48x48 pixels. 

![Dataset images.png](https://github.com/mermora97/Facial-Emotion-Recognition/blob/main/Dataset%20images.png)
